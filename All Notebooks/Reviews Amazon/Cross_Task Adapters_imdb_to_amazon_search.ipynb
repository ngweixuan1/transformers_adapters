{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Cross_Task Adapters_imdb_to_amazon_search.ipynb","provenance":[{"file_id":"1LUlIxyhe-rvn4ORxdjRHKe1UEJpBVgX8","timestamp":1619772562451},{"file_id":"1gxddX4gig3SnW4xir9KViJ7uhp3P7mBM","timestamp":1619621809460},{"file_id":"1jkthCAgVbiEwPYMJ2FQzHtrpiI71P7np","timestamp":1619613656824}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f4dce1b037a94618b0174938b78bc1eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4b138c7ce240402299e95bd12c7846db","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c0424561222c4771bc04cdb41ebb4e12","IPY_MODEL_a56f6cc916424a61a4e1afa054f5462e"]}},"4b138c7ce240402299e95bd12c7846db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c0424561222c4771bc04cdb41ebb4e12":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b25b44718fc49068032ee9f3907b49a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c24a5c2d1d064315b1571f00eb334d35"}},"a56f6cc916424a61a4e1afa054f5462e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c8bac0d466ca4230b2fae09c055942cc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:03&lt;00:00, 264kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b2990278990a417580b32b074d287c8e"}},"4b25b44718fc49068032ee9f3907b49a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c24a5c2d1d064315b1571f00eb334d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c8bac0d466ca4230b2fae09c055942cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b2990278990a417580b32b074d287c8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8aa1f251780a4e52bf558406f6f74322":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2b5c771a9ba04b73889bd426f6f111ca","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0d564206b27049caa582ae3b5d8f6c07","IPY_MODEL_d2241fa333324903abe403375de872f8"]}},"2b5c771a9ba04b73889bd426f6f111ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0d564206b27049caa582ae3b5d8f6c07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b5490d338714ce98d41941daf55dead","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5d1ed8586dbe45e5b6319e69e11d1df1"}},"d2241fa333324903abe403375de872f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5f0e762b8aee43d19eebabb84007c1e1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:02&lt;00:00, 194kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f9f5fa29547411f9ecea167a7a3de9b"}},"4b5490d338714ce98d41941daf55dead":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5d1ed8586dbe45e5b6319e69e11d1df1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f0e762b8aee43d19eebabb84007c1e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7f9f5fa29547411f9ecea167a7a3de9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5396b7cf985140d3b85be846fb9743c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_02536d90c31b4e939766fa7784220ebb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a9866858e5e04b0385821de002293feb","IPY_MODEL_02eea218fbf846e4ac9216f8ba9586bb"]}},"02536d90c31b4e939766fa7784220ebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a9866858e5e04b0385821de002293feb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0f276f27aa21401786085f7ae83e020b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1355863,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1355863,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d428887eeb0b43deb0c2345ed144aa58"}},"02eea218fbf846e4ac9216f8ba9586bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_87df0c4f14904d0ca4fa9ac9aa1423ce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.36M/1.36M [00:00&lt;00:00, 2.38MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_54e89ce72de94f01ad65bd6937228e94"}},"0f276f27aa21401786085f7ae83e020b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d428887eeb0b43deb0c2345ed144aa58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"87df0c4f14904d0ca4fa9ac9aa1423ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"54e89ce72de94f01ad65bd6937228e94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lZj-G8LIXLpj","executionInfo":{"status":"ok","timestamp":1619922533654,"user_tz":-480,"elapsed":17000,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}},"outputId":"891d0d49-4056-4fb1-91da-aeba0a4e032a"},"source":["# mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# log_dir='/content/gdrive/MyDrive/DL_Project/runs'\n","# !mkdir -p {log_dir}\n","# !mv ./runs/* {log_dir}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D61LwET3WrOD","executionInfo":{"status":"ok","timestamp":1619922653427,"user_tz":-480,"elapsed":136766,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}},"outputId":"7130af05-9817-46a3-99d8-fa630fef2342"},"source":["!pip install -U git+https://github.com/Adapter-Hub/adapter-transformers.git@v2\n","!pip install torch==1.7.1\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/Adapter-Hub/adapter-transformers.git@v2\n","  Cloning https://github.com/Adapter-Hub/adapter-transformers.git (to revision v2) to /tmp/pip-req-build-um37a1sp\n","  Running command git clone -q https://github.com/Adapter-Hub/adapter-transformers.git /tmp/pip-req-build-um37a1sp\n","  Running command git checkout -b v2 --track origin/v2\n","  Switched to a new branch 'v2'\n","  Branch 'v2' set up to track remote branch 'v2' from 'origin'.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 12.6MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0a1) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0a1) (20.9)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0a1) (2.23.0)\n","Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0a1) (3.10.1)\n","Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0a1) (4.41.1)\n","Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0a1) (3.0.12)\n","Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from adapter-transformers==2.0.0a1) (1.19.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 52.6MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->adapter-transformers==2.0.0a1) (2.4.7)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.0.0a1) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.0.0a1) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.0.0a1) (2.10)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->adapter-transformers==2.0.0a1) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->adapter-transformers==2.0.0a1) (3.4.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->adapter-transformers==2.0.0a1) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.0.0a1) (1.15.0)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.0.0a1) (7.1.2)\n","Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->adapter-transformers==2.0.0a1) (1.0.1)\n","Building wheels for collected packages: adapter-transformers\n","  Building wheel for adapter-transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for adapter-transformers: filename=adapter_transformers-2.0.0a1-cp37-none-any.whl size=2097547 sha256=d767aa1272a6c7d529c333c966ac4eafc2d9f6331d6d7b7bd412acf2ecdcd8c1\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-zhhkxt7b/wheels/11/c5/35/7017ef1a9923a73e9d8071801894534ab1fa662e38e23b78f1\n","Successfully built adapter-transformers\n","Installing collected packages: tokenizers, sacremoses, adapter-transformers\n","Successfully installed adapter-transformers-2.0.0a1 sacremoses-0.0.45 tokenizers-0.10.2\n","Collecting torch==1.7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 24kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.5)\n","\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n","Installing collected packages: torch\n","  Found existing installation: torch 1.8.1+cu101\n","    Uninstalling torch-1.8.1+cu101:\n","      Successfully uninstalled torch-1.8.1+cu101\n","Successfully installed torch-1.7.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPpEKGrz_iNF","executionInfo":{"status":"ok","timestamp":1619922656760,"user_tz":-480,"elapsed":140095,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}},"outputId":"b632e1ec-4410-4c42-8733-08b0ccac2e42"},"source":["import json, gc\n","import urllib.request\n","import numpy as np\n","import pandas as pd\n","import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","\n","from transformers import (\n","    MODEL_WITH_LM_HEAD_MAPPING,\n","    WEIGHTS_NAME,\n","    AdamW,\n","    AutoConfig,\n","    AutoModelWithLMHead,\n","    AutoTokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    get_linear_schedule_with_warmup,\n","    RobertaTokenizer\n",")\n","from transformers import RobertaForSequenceClassification, RobertaModelWithHeads, RobertaConfig\n","from transformers import TrainingArguments, Trainer, EvalPrediction\n","\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","from transformers import EarlyStoppingCallback\n","from transformers.integrations import TensorBoardCallback\n","from tensorflow import summary\n","import tensorflow\n","%load_ext tensorboard\n","import datetime"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using device: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wwRHoyT1Kwyl"},"source":["Dataset and Tokenizers"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163,"referenced_widgets":["f4dce1b037a94618b0174938b78bc1eb","4b138c7ce240402299e95bd12c7846db","c0424561222c4771bc04cdb41ebb4e12","a56f6cc916424a61a4e1afa054f5462e","4b25b44718fc49068032ee9f3907b49a","c24a5c2d1d064315b1571f00eb334d35","c8bac0d466ca4230b2fae09c055942cc","b2990278990a417580b32b074d287c8e","8aa1f251780a4e52bf558406f6f74322","2b5c771a9ba04b73889bd426f6f111ca","0d564206b27049caa582ae3b5d8f6c07","d2241fa333324903abe403375de872f8","4b5490d338714ce98d41941daf55dead","5d1ed8586dbe45e5b6319e69e11d1df1","5f0e762b8aee43d19eebabb84007c1e1","7f9f5fa29547411f9ecea167a7a3de9b","5396b7cf985140d3b85be846fb9743c0","02536d90c31b4e939766fa7784220ebb","a9866858e5e04b0385821de002293feb","02eea218fbf846e4ac9216f8ba9586bb","0f276f27aa21401786085f7ae83e020b","d428887eeb0b43deb0c2345ed144aa58","87df0c4f14904d0ca4fa9ac9aa1423ce","54e89ce72de94f01ad65bd6937228e94"]},"id":"tTjqSHp8IjOp","executionInfo":{"status":"ok","timestamp":1619922660935,"user_tz":-480,"elapsed":144263,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}},"outputId":"6b74a10d-6c9e-4eab-d0fa-bc5df299150e"},"source":["DATASET = 'amazon'\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4dce1b037a94618b0174938b78bc1eb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8aa1f251780a4e52bf558406f6f74322","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5396b7cf985140d3b85be846fb9743c0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RoTj34hLH6Ui","executionInfo":{"status":"ok","timestamp":1619922882833,"user_tz":-480,"elapsed":166405,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}},"outputId":"eef687dc-7449-44ee-d1ab-84f9a6472731"},"source":["print(tokenizer.bos_token_id, tokenizer.bos_token, tokenizer.eos_token_id, tokenizer.eos_token)\n","MAX_SEQUENCE_LENGTH = 512\n","\n","\n","def load_and_tokenize(url, label2id={}, count_label = 0, tokenizer=tokenizer):\n","  # tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","  block_size=512\n","  dataframe = []\n","  with urllib.request.urlopen(url) as f:\n","    for line in f:\n","      doc = json.loads(line.decode('utf-8'))['text']\n","      tokenized_text = tokenizer(doc, max_length=MAX_SEQUENCE_LENGTH, truncation=True, padding=\"max_length\")\n","      label = json.loads(line.decode('utf-8'))['label']\n","      \n","      if label not in label2id:\n","        label2id[label] = count_label\n","        count_label +=1\n","      tokenized_text['labels'] = torch.tensor(label2id[label])\n","      tokenized_text['input_ids'] = torch.tensor(tokenized_text['input_ids'])\n","      tokenized_text['attention_mask'] = torch.tensor(tokenized_text['attention_mask'])\n","      dataframe.append(tokenized_text)\n","  return dataframe, count_label, label2id\n","\n","train,count_label, label2id = load_and_tokenize(\"https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/\"+DATASET+\"/train.jsonl\", tokenizer=tokenizer)\n","dev,count_label,label2id = load_and_tokenize(\"https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/\"+DATASET+\"/dev.jsonl\",label2id,  count_label, tokenizer=tokenizer)\n","test,count_label,label2id = load_and_tokenize(\"https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/\"+DATASET+\"/test.jsonl\",label2id,  count_label, tokenizer=tokenizer)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0 <s> 2 </s>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ng0MCo9NH6XJ","executionInfo":{"status":"ok","timestamp":1619922882838,"user_tz":-480,"elapsed":166403,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}}},"source":["# config"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKy95v85v3A4","executionInfo":{"status":"ok","timestamp":1619922882838,"user_tz":-480,"elapsed":166399,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}},"outputId":"0312535a-682d-4786-afa1-d54f8c688860"},"source":["train[0]\n","print(len(train))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["115251\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hTNu7bBNUJ7S"},"source":["Folder to save results"]},{"cell_type":"code","metadata":{"id":"U86TlZD8UJ7X","executionInfo":{"status":"ok","timestamp":1619922882839,"user_tz":-480,"elapsed":166397,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}}},"source":["RESULTS_DIR = f\"\"\"/content/drive/My Drive/DL_Project/results/cross-task/imdb_to_amazon/search/\"\"\""],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JAeAUGarJxb6"},"source":["General packages and functions"]},{"cell_type":"code","metadata":{"id":"LUAcaKKFJGeX","executionInfo":{"status":"ok","timestamp":1619922882839,"user_tz":-480,"elapsed":166394,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}}},"source":["from transformers import TrainingArguments, Trainer, EvalPrediction\n","def compute_accuracy(p: EvalPrediction):\n","    labels = p.label_ids\n","    preds = np.argmax(p.predictions, axis=1)\n","    acc = accuracy_score(labels, preds)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","from transformers import AdapterType, AdapterConfig"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXIo9sQ2M43p","executionInfo":{"status":"ok","timestamp":1619922882840,"user_tz":-480,"elapsed":166392,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}}},"source":["def run_model(hyperparams_dict, model_name=\"roberta-base\",task_name=\"myown\",adapter_name=None,adapter_config=None,seed=999):\n","    global model\n","    config = RobertaConfig.from_pretrained(\n","        model_name,\n","        num_labels=len(label2id),\n","    )\n","    model = RobertaModelWithHeads.from_pretrained(\n","        model_name,\n","        config=config,\n","    )\n","\n","    if torch.cuda.is_available():\n","      model = model.to(\"cuda\")\n","    id2label= {v: k for k, v in label2id.items()}\n","    model.load_adapter(\"/content/drive/My Drive/DL_Project/results/imdb/imdb_base_finetune\",adapter_type = AdapterType.text_task, config=None, model_name=\"roberta-base\", load_as=\"imdb_base_finetune\", with_head=False)\n","    model.freeze_model(True)\n","    '''\n","    if adapter_name:\n","      # add a new adapter\n","      if adapter_config:\n","        model.add_adapter(\n","            task_name,\n","            ##### remove AdapterType argument for v2 #####\n","            #AdapterType.text_task,\n","            config=adapter_config\n","        )\n","      else:\n","        model.add_adapter(\n","            task_name,\n","            ##### remove AdapterType argument for v2 #####\n","            #AdapterType.text_task \n","        )\n","      # Enable adapter training\n","      model.train_adapter([task_name])\n","      ''' \n","    # Add a matching classification head\n","    model.add_classification_head(\n","        task_name,\n","        num_labels=len(label2id),\n","        id2label=id2label,\n","        layers=2\n","      )\n","    # train, dev, test = get_datasets(tokenizer)\n","    training_args = TrainingArguments(\n","        learning_rate=hyperparams_dict['learning_rate'],\n","        num_train_epochs=hyperparams_dict['num_train_epochs'],\n","        per_device_train_batch_size=hyperparams_dict['per_device_train_batch_size'],\n","        per_device_eval_batch_size=hyperparams_dict['per_device_eval_batch_size'],\n","        logging_steps=hyperparams_dict['logging_steps'],\n","        save_steps=hyperparams_dict['save_steps'],\n","        output_dir='./models/'+task_name,\n","        overwrite_output_dir=True,\n","        do_train=True,\n","        do_eval=True,\n","        do_predict=True,\n","        evaluation_strategy='steps', # use evaluation_strategy='epoch' for v2, evaluation_strategy='step' for large dataset\n","        # The next line is important to ensure the dataset labels are properly passed to the model\n","        remove_unused_columns=False,\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"loss\",\n","        greater_is_better=False,\n","        seed=int(seed)\n","    )\n","\n","    # tensor_board = TensorBoardCallback()\n","    ##### Early Stopping #####\n","    es = EarlyStoppingCallback(early_stopping_patience=10, early_stopping_threshold=0.0)\n","    if adapter_name:\n","      trainer = Trainer(\n","          model=model,\n","          args=training_args,\n","          train_dataset=train,\n","          eval_dataset=dev,\n","          compute_metrics=compute_accuracy,\n","          callbacks=[es],\n","          adapter_names=[adapter_name]   \n","      )\n","    else:\n","      trainer = Trainer(\n","          model=model,\n","          args=training_args,\n","          train_dataset=train,\n","          eval_dataset=dev,\n","          callbacks=[es],\n","          compute_metrics=compute_accuracy\n","      )\n","    trainer.train()\n","\n","    ##### Explicitly set active adapter to pass it in model forward pass,             #####\n","    ##### otherwise the previous setting adapter_names=[adapter_name] not work for v2 #####\n","    if adapter_name:\n","      trainer.model.set_active_adapters(adapter_name)\n","    model.freeze_model(False)\n","    _, _, metrics = trainer.predict(dev)\n","    metrics['seed'] = seed\n","    dev_results.append(pd.DataFrame.from_dict(metrics, orient='index').T)\n","\n","    metrics['seed'] = seed\n","    _, _, metrics = trainer.predict(test)\n","    test_results.append(pd.DataFrame.from_dict(metrics, orient='index').T)\n","    \n","    filepath = RESULTS_DIR + \"\"\"{}_{}_{}\"\"\".format(task_name,seed,timestamp.strftime(\"%Y-%m-%dT%H_%M_%S\"))\n","    filepath = filepath + '_' + str(hyperparams_dict['learning_rate']) \\\n","                + '_' + str(hyperparams_dict['num_train_epochs']) \\\n","                + '_' + str(hyperparams_dict['per_device_train_batch_size'] ) \\\n","                + '_' + str(hyperparams_dict['per_device_eval_batch_size']) \\\n","                + '_' + str(hyperparams_dict['logging_steps']) \\\n","                + '_' + str(hyperparams_dict['save_steps'])\n","    trainer.save_model(filepath)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CH189097M2Aq"},"source":["### Model: RoBERTa Base\n","### Task: Reviews\n","### Finetuning: Standard with classification head\n","### Adapter: None (None /Custom /Default / Houlsby / Pfeiffer)"]},{"cell_type":"code","metadata":{"id":"4lqaOie3M2Aq","executionInfo":{"status":"ok","timestamp":1619923230730,"user_tz":-480,"elapsed":873,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}}},"source":["# MODEL_NAME = \"allenai/cs_roberta_base\"\n","MODEL_NAME = \"roberta-base\"\n","TASK_NAME = \"imdb_base_cross\" # cit_intent_base_myownadapter / cit_intent_base_pfieffer\n","ADAPTER_NAME = \"imdb_base_finetune\" # None pfieffer / cit_intent_base_finetune\n","\n","\n","ADAPTER_CONFIG = None # leave ADAPTER_CONFIG as None to default adapter\n","# ADAPTER_CONFIG = AdapterConfig.load( # comment out if using default adapter\n","#     # adapter_args.adapter_config,\n","#     # non_linearity=adapter_args.adapter_non_linearity,\n","#     # reduction_factor=adapter_args.adapter_reduction_factor,\n","#     ADAPTER_NAME, \n","#     # non_linearity=adapter_args.adapter_non_linearity,\n","#     reduction_factor=12\n","# )\n","\n","# hyperparameters search\n","hyperparameters_dict = {'learning_rate':2e-5,'num_train_epochs':10,'per_device_train_batch_size':16,'per_device_eval_batch_size':16,'logging_steps':200,'save_steps':200} # hyperparameters for standard finetuning\n","\n","HYPERPARAMETERS_SEARCH = [{'learning_rate':1e-4,'num_train_epochs':10,'per_device_train_batch_size':16,'per_device_eval_batch_size':16,'logging_steps':1000,'save_steps':1000},\n","                          {'learning_rate':8e-5,'num_train_epochs':10,'per_device_train_batch_size':16,'per_device_eval_batch_size':16,'logging_steps':1000,'save_steps':1000},\n","                          {'learning_rate':2e-5,'num_train_epochs':10,'per_device_train_batch_size':16,'per_device_eval_batch_size':16,'logging_steps':1000,'save_steps':1000},\n","                          ]\n","\n","\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"u52XtNy1M2Aq","executionInfo":{"status":"ok","timestamp":1619935070356,"user_tz":-480,"elapsed":11838000,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}},"outputId":"302a5c94-ff2a-4d24-864c-fb56ba14d494"},"source":["\n","\n","from datetime import datetime, timedelta\n","timestamp = datetime.today() + timedelta(hours=8)\n","tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n","print(tokenizer.bos_token_id, tokenizer.bos_token, tokenizer.eos_token_id, tokenizer.eos_token)\n","\n","seeds = [42,1,2]\n","dev_results = []\n","test_results = []\n","print('seeds:', seeds)\n","\n","if not ADAPTER_NAME:\n","    # if using adapter, loop by HYPERPARAMETERS_SEARCH defined above\n","    for hyperparameters_dict in HYPERPARAMETERS_SEARCH:\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        print(hyperparameters_dict)\n","        run_model(hyperparameters_dict, model_name=MODEL_NAME,task_name=TASK_NAME,adapter_name=ADAPTER_NAME,adapter_config=ADAPTER_CONFIG)\n","else: # if not using adapter, assume standard finetuning and loop by seeds\n","    for seed in seeds:\n","        print(type(int(seed)))\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        print(hyperparameters_dict)\n","        run_model(hyperparameters_dict, model_name=MODEL_NAME,task_name=TASK_NAME,adapter_name=ADAPTER_NAME,adapter_config=ADAPTER_CONFIG, seed=seed)\n","\n","dev_df = pd.concat(dev_results)\n","test_df = pd.concat(test_results)\n","\n","\n","# UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior._warn_prf(average, modifier, msg_start, len(result))\n","## some labels in y_test don't appear in y_pred. \n","\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["0 <s> 2 </s>\n","seeds: [42, 1, 2]\n","<class 'int'>\n","{'learning_rate': 2e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'logging_steps': 200, 'save_steps': 200}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='5200' max='72040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 5200/72040 39:21 < 8:26:00, 2.20 it/s, Epoch 0/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>200</td>\n","      <td>0.382900</td>\n","      <td>0.358625</td>\n","      <td>0.853400</td>\n","      <td>0.460451</td>\n","      <td>0.426700</td>\n","      <td>0.500000</td>\n","      <td>50.165500</td>\n","      <td>99.670000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.345200</td>\n","      <td>0.354519</td>\n","      <td>0.853600</td>\n","      <td>0.464549</td>\n","      <td>0.726927</td>\n","      <td>0.501812</td>\n","      <td>50.195400</td>\n","      <td>99.611000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.344000</td>\n","      <td>0.359269</td>\n","      <td>0.853400</td>\n","      <td>0.465815</td>\n","      <td>0.676983</td>\n","      <td>0.502260</td>\n","      <td>50.159500</td>\n","      <td>99.682000</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.351900</td>\n","      <td>0.342759</td>\n","      <td>0.855200</td>\n","      <td>0.527483</td>\n","      <td>0.702721</td>\n","      <td>0.532692</td>\n","      <td>50.250700</td>\n","      <td>99.501000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.368800</td>\n","      <td>0.349331</td>\n","      <td>0.855200</td>\n","      <td>0.476916</td>\n","      <td>0.827683</td>\n","      <td>0.507834</td>\n","      <td>50.230100</td>\n","      <td>99.542000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.344000</td>\n","      <td>0.341339</td>\n","      <td>0.856200</td>\n","      <td>0.497159</td>\n","      <td>0.762236</td>\n","      <td>0.517459</td>\n","      <td>50.144400</td>\n","      <td>99.712000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.344400</td>\n","      <td>0.340312</td>\n","      <td>0.856000</td>\n","      <td>0.504068</td>\n","      <td>0.738413</td>\n","      <td>0.520732</td>\n","      <td>50.230200</td>\n","      <td>99.542000</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.331800</td>\n","      <td>0.346420</td>\n","      <td>0.856800</td>\n","      <td>0.512474</td>\n","      <td>0.745273</td>\n","      <td>0.525155</td>\n","      <td>50.220700</td>\n","      <td>99.561000</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.340500</td>\n","      <td>0.339875</td>\n","      <td>0.858200</td>\n","      <td>0.524307</td>\n","      <td>0.757642</td>\n","      <td>0.531625</td>\n","      <td>50.192800</td>\n","      <td>99.616000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.343300</td>\n","      <td>0.348160</td>\n","      <td>0.857000</td>\n","      <td>0.504636</td>\n","      <td>0.769394</td>\n","      <td>0.521318</td>\n","      <td>50.198700</td>\n","      <td>99.604000</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.350500</td>\n","      <td>0.334897</td>\n","      <td>0.863200</td>\n","      <td>0.599186</td>\n","      <td>0.745578</td>\n","      <td>0.578620</td>\n","      <td>50.222600</td>\n","      <td>99.557000</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.353400</td>\n","      <td>0.337147</td>\n","      <td>0.861800</td>\n","      <td>0.559061</td>\n","      <td>0.768024</td>\n","      <td>0.551812</td>\n","      <td>50.136800</td>\n","      <td>99.727000</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.337600</td>\n","      <td>0.334085</td>\n","      <td>0.863000</td>\n","      <td>0.576283</td>\n","      <td>0.762714</td>\n","      <td>0.562684</td>\n","      <td>50.159000</td>\n","      <td>99.683000</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.345500</td>\n","      <td>0.337885</td>\n","      <td>0.860400</td>\n","      <td>0.539440</td>\n","      <td>0.775918</td>\n","      <td>0.540258</td>\n","      <td>50.218100</td>\n","      <td>99.566000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.343200</td>\n","      <td>0.340606</td>\n","      <td>0.859000</td>\n","      <td>0.525924</td>\n","      <td>0.772954</td>\n","      <td>0.532658</td>\n","      <td>50.175600</td>\n","      <td>99.650000</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.348700</td>\n","      <td>0.330536</td>\n","      <td>0.865400</td>\n","      <td>0.605256</td>\n","      <td>0.759615</td>\n","      <td>0.582734</td>\n","      <td>50.234600</td>\n","      <td>99.533000</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.327200</td>\n","      <td>0.336085</td>\n","      <td>0.862400</td>\n","      <td>0.568713</td>\n","      <td>0.763930</td>\n","      <td>0.557813</td>\n","      <td>50.173700</td>\n","      <td>99.654000</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.326300</td>\n","      <td>0.334376</td>\n","      <td>0.862600</td>\n","      <td>0.567998</td>\n","      <td>0.767414</td>\n","      <td>0.557366</td>\n","      <td>50.194400</td>\n","      <td>99.613000</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.332400</td>\n","      <td>0.333117</td>\n","      <td>0.866400</td>\n","      <td>0.607816</td>\n","      <td>0.766224</td>\n","      <td>0.584450</td>\n","      <td>50.194700</td>\n","      <td>99.612000</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.340000</td>\n","      <td>0.334178</td>\n","      <td>0.863600</td>\n","      <td>0.572474</td>\n","      <td>0.774877</td>\n","      <td>0.560211</td>\n","      <td>50.250100</td>\n","      <td>99.502000</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.328200</td>\n","      <td>0.345085</td>\n","      <td>0.860600</td>\n","      <td>0.538574</td>\n","      <td>0.781772</td>\n","      <td>0.539810</td>\n","      <td>50.192100</td>\n","      <td>99.617000</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.348000</td>\n","      <td>0.334075</td>\n","      <td>0.861200</td>\n","      <td>0.564049</td>\n","      <td>0.753902</td>\n","      <td>0.554850</td>\n","      <td>50.187800</td>\n","      <td>99.626000</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.317400</td>\n","      <td>0.343596</td>\n","      <td>0.860000</td>\n","      <td>0.538120</td>\n","      <td>0.770440</td>\n","      <td>0.539459</td>\n","      <td>50.181800</td>\n","      <td>99.638000</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.350500</td>\n","      <td>0.335592</td>\n","      <td>0.861800</td>\n","      <td>0.563670</td>\n","      <td>0.762082</td>\n","      <td>0.554637</td>\n","      <td>50.209700</td>\n","      <td>99.582000</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.345100</td>\n","      <td>0.330702</td>\n","      <td>0.865000</td>\n","      <td>0.617150</td>\n","      <td>0.748240</td>\n","      <td>0.592104</td>\n","      <td>50.166100</td>\n","      <td>99.669000</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.330900</td>\n","      <td>0.332351</td>\n","      <td>0.862000</td>\n","      <td>0.583748</td>\n","      <td>0.745546</td>\n","      <td>0.567748</td>\n","      <td>50.191300</td>\n","      <td>99.619000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='1876' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [313/313 05:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["<class 'int'>\n","{'learning_rate': 2e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'logging_steps': 200, 'save_steps': 200}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='7600' max='72040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 7600/72040 57:29 < 8:07:38, 2.20 it/s, Epoch 1/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>200</td>\n","      <td>0.365500</td>\n","      <td>0.355833</td>\n","      <td>0.853200</td>\n","      <td>0.460393</td>\n","      <td>0.426685</td>\n","      <td>0.499883</td>\n","      <td>50.168400</td>\n","      <td>99.664000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.351700</td>\n","      <td>0.349195</td>\n","      <td>0.854600</td>\n","      <td>0.472804</td>\n","      <td>0.802426</td>\n","      <td>0.505788</td>\n","      <td>50.208200</td>\n","      <td>99.585000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.354800</td>\n","      <td>0.349208</td>\n","      <td>0.854600</td>\n","      <td>0.475393</td>\n","      <td>0.771318</td>\n","      <td>0.506918</td>\n","      <td>50.246900</td>\n","      <td>99.509000</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.354800</td>\n","      <td>0.350572</td>\n","      <td>0.854000</td>\n","      <td>0.471281</td>\n","      <td>0.735003</td>\n","      <td>0.504871</td>\n","      <td>50.186600</td>\n","      <td>99.628000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.351800</td>\n","      <td>0.353084</td>\n","      <td>0.855600</td>\n","      <td>0.485931</td>\n","      <td>0.780064</td>\n","      <td>0.512023</td>\n","      <td>50.168300</td>\n","      <td>99.665000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.366000</td>\n","      <td>0.341320</td>\n","      <td>0.856000</td>\n","      <td>0.513091</td>\n","      <td>0.725679</td>\n","      <td>0.525251</td>\n","      <td>50.185900</td>\n","      <td>99.630000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.319000</td>\n","      <td>0.344366</td>\n","      <td>0.856400</td>\n","      <td>0.508863</td>\n","      <td>0.741117</td>\n","      <td>0.523226</td>\n","      <td>50.198600</td>\n","      <td>99.604000</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.358700</td>\n","      <td>0.339272</td>\n","      <td>0.856800</td>\n","      <td>0.511355</td>\n","      <td>0.747276</td>\n","      <td>0.524590</td>\n","      <td>50.190800</td>\n","      <td>99.620000</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.346800</td>\n","      <td>0.335764</td>\n","      <td>0.861000</td>\n","      <td>0.577814</td>\n","      <td>0.740195</td>\n","      <td>0.563773</td>\n","      <td>50.169800</td>\n","      <td>99.662000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.340600</td>\n","      <td>0.342604</td>\n","      <td>0.856400</td>\n","      <td>0.506590</td>\n","      <td>0.745286</td>\n","      <td>0.522096</td>\n","      <td>50.191100</td>\n","      <td>99.619000</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.359600</td>\n","      <td>0.337732</td>\n","      <td>0.858200</td>\n","      <td>0.526435</td>\n","      <td>0.754037</td>\n","      <td>0.532755</td>\n","      <td>50.210100</td>\n","      <td>99.581000</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.332800</td>\n","      <td>0.336379</td>\n","      <td>0.859400</td>\n","      <td>0.538680</td>\n","      <td>0.758293</td>\n","      <td>0.539672</td>\n","      <td>50.221400</td>\n","      <td>99.559000</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.324100</td>\n","      <td>0.339660</td>\n","      <td>0.860400</td>\n","      <td>0.561537</td>\n","      <td>0.746502</td>\n","      <td>0.553252</td>\n","      <td>50.188900</td>\n","      <td>99.624000</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.331900</td>\n","      <td>0.347139</td>\n","      <td>0.857800</td>\n","      <td>0.521889</td>\n","      <td>0.752913</td>\n","      <td>0.530260</td>\n","      <td>50.249200</td>\n","      <td>99.504000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.334600</td>\n","      <td>0.338340</td>\n","      <td>0.861200</td>\n","      <td>0.556678</td>\n","      <td>0.762424</td>\n","      <td>0.550331</td>\n","      <td>50.226800</td>\n","      <td>99.548000</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.341800</td>\n","      <td>0.335468</td>\n","      <td>0.860400</td>\n","      <td>0.537400</td>\n","      <td>0.779999</td>\n","      <td>0.539128</td>\n","      <td>50.239800</td>\n","      <td>99.523000</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.337300</td>\n","      <td>0.341424</td>\n","      <td>0.858000</td>\n","      <td>0.515450</td>\n","      <td>0.771389</td>\n","      <td>0.526988</td>\n","      <td>50.186600</td>\n","      <td>99.628000</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.330100</td>\n","      <td>0.339344</td>\n","      <td>0.860200</td>\n","      <td>0.547242</td>\n","      <td>0.759686</td>\n","      <td>0.544660</td>\n","      <td>50.150400</td>\n","      <td>99.700000</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.349700</td>\n","      <td>0.340123</td>\n","      <td>0.858200</td>\n","      <td>0.521074</td>\n","      <td>0.763833</td>\n","      <td>0.529930</td>\n","      <td>50.235300</td>\n","      <td>99.532000</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.321300</td>\n","      <td>0.345017</td>\n","      <td>0.860000</td>\n","      <td>0.532969</td>\n","      <td>0.780758</td>\n","      <td>0.536634</td>\n","      <td>50.145500</td>\n","      <td>99.710000</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.354100</td>\n","      <td>0.332414</td>\n","      <td>0.863800</td>\n","      <td>0.576168</td>\n","      <td>0.772607</td>\n","      <td>0.562588</td>\n","      <td>50.157600</td>\n","      <td>99.686000</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.336300</td>\n","      <td>0.334667</td>\n","      <td>0.862000</td>\n","      <td>0.571006</td>\n","      <td>0.756491</td>\n","      <td>0.559274</td>\n","      <td>50.184700</td>\n","      <td>99.632000</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.327800</td>\n","      <td>0.334644</td>\n","      <td>0.861600</td>\n","      <td>0.624212</td>\n","      <td>0.724860</td>\n","      <td>0.599151</td>\n","      <td>50.171300</td>\n","      <td>99.659000</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.330300</td>\n","      <td>0.344550</td>\n","      <td>0.860000</td>\n","      <td>0.530872</td>\n","      <td>0.785644</td>\n","      <td>0.535504</td>\n","      <td>50.164000</td>\n","      <td>99.673000</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.331300</td>\n","      <td>0.330835</td>\n","      <td>0.863400</td>\n","      <td>0.577520</td>\n","      <td>0.766064</td>\n","      <td>0.563484</td>\n","      <td>50.130100</td>\n","      <td>99.741000</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.325700</td>\n","      <td>0.336931</td>\n","      <td>0.863600</td>\n","      <td>0.561556</td>\n","      <td>0.792519</td>\n","      <td>0.553432</td>\n","      <td>50.176100</td>\n","      <td>99.649000</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.339800</td>\n","      <td>0.334630</td>\n","      <td>0.865000</td>\n","      <td>0.579902</td>\n","      <td>0.783051</td>\n","      <td>0.564986</td>\n","      <td>50.207400</td>\n","      <td>99.587000</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>0.341500</td>\n","      <td>0.329735</td>\n","      <td>0.865200</td>\n","      <td>0.600479</td>\n","      <td>0.761892</td>\n","      <td>0.579227</td>\n","      <td>50.188100</td>\n","      <td>99.625000</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>0.321900</td>\n","      <td>0.340606</td>\n","      <td>0.862400</td>\n","      <td>0.557698</td>\n","      <td>0.779392</td>\n","      <td>0.551034</td>\n","      <td>50.163500</td>\n","      <td>99.674000</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.331100</td>\n","      <td>0.330935</td>\n","      <td>0.864600</td>\n","      <td>0.594389</td>\n","      <td>0.761704</td>\n","      <td>0.574921</td>\n","      <td>50.175400</td>\n","      <td>99.650000</td>\n","    </tr>\n","    <tr>\n","      <td>6200</td>\n","      <td>0.330600</td>\n","      <td>0.335386</td>\n","      <td>0.863200</td>\n","      <td>0.563989</td>\n","      <td>0.781744</td>\n","      <td>0.554892</td>\n","      <td>50.158100</td>\n","      <td>99.685000</td>\n","    </tr>\n","    <tr>\n","      <td>6400</td>\n","      <td>0.322700</td>\n","      <td>0.344531</td>\n","      <td>0.862200</td>\n","      <td>0.544877</td>\n","      <td>0.801819</td>\n","      <td>0.543572</td>\n","      <td>50.149600</td>\n","      <td>99.702000</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>0.322200</td>\n","      <td>0.335051</td>\n","      <td>0.865200</td>\n","      <td>0.602014</td>\n","      <td>0.760557</td>\n","      <td>0.580357</td>\n","      <td>50.110800</td>\n","      <td>99.779000</td>\n","    </tr>\n","    <tr>\n","      <td>6800</td>\n","      <td>0.349200</td>\n","      <td>0.338618</td>\n","      <td>0.863200</td>\n","      <td>0.560270</td>\n","      <td>0.788143</td>\n","      <td>0.552633</td>\n","      <td>50.174500</td>\n","      <td>99.652000</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.322300</td>\n","      <td>0.351202</td>\n","      <td>0.860000</td>\n","      <td>0.523368</td>\n","      <td>0.807622</td>\n","      <td>0.531549</td>\n","      <td>50.161700</td>\n","      <td>99.678000</td>\n","    </tr>\n","    <tr>\n","      <td>7200</td>\n","      <td>0.354500</td>\n","      <td>0.340344</td>\n","      <td>0.862400</td>\n","      <td>0.543015</td>\n","      <td>0.811429</td>\n","      <td>0.542560</td>\n","      <td>50.166300</td>\n","      <td>99.668000</td>\n","    </tr>\n","    <tr>\n","      <td>7400</td>\n","      <td>0.335200</td>\n","      <td>0.336133</td>\n","      <td>0.863200</td>\n","      <td>0.561206</td>\n","      <td>0.786459</td>\n","      <td>0.553198</td>\n","      <td>50.206500</td>\n","      <td>99.589000</td>\n","    </tr>\n","    <tr>\n","      <td>7600</td>\n","      <td>0.319000</td>\n","      <td>0.338489</td>\n","      <td>0.864000</td>\n","      <td>0.563772</td>\n","      <td>0.795067</td>\n","      <td>0.554796</td>\n","      <td>50.182400</td>\n","      <td>99.636000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='1876' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [313/313 05:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["<class 'int'>\n","{'learning_rate': 2e-05, 'num_train_epochs': 10, 'per_device_train_batch_size': 16, 'per_device_eval_batch_size': 16, 'logging_steps': 200, 'save_steps': 200}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='11200' max='72040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11200/72040 1:24:41 < 7:40:06, 2.20 it/s, Epoch 1/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Runtime</th>\n","      <th>Samples Per Second</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>200</td>\n","      <td>0.410500</td>\n","      <td>0.360015</td>\n","      <td>0.853200</td>\n","      <td>0.460393</td>\n","      <td>0.426685</td>\n","      <td>0.499883</td>\n","      <td>50.151700</td>\n","      <td>99.698000</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.363000</td>\n","      <td>0.351647</td>\n","      <td>0.854600</td>\n","      <td>0.486706</td>\n","      <td>0.722329</td>\n","      <td>0.512002</td>\n","      <td>50.231400</td>\n","      <td>99.539000</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.354900</td>\n","      <td>0.346955</td>\n","      <td>0.855000</td>\n","      <td>0.502356</td>\n","      <td>0.713661</td>\n","      <td>0.519581</td>\n","      <td>50.167500</td>\n","      <td>99.666000</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.336200</td>\n","      <td>0.347286</td>\n","      <td>0.854800</td>\n","      <td>0.492863</td>\n","      <td>0.719284</td>\n","      <td>0.514944</td>\n","      <td>50.161100</td>\n","      <td>99.679000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.347800</td>\n","      <td>0.350520</td>\n","      <td>0.854800</td>\n","      <td>0.485566</td>\n","      <td>0.734606</td>\n","      <td>0.511554</td>\n","      <td>50.167000</td>\n","      <td>99.667000</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.344000</td>\n","      <td>0.350823</td>\n","      <td>0.854600</td>\n","      <td>0.485475</td>\n","      <td>0.725015</td>\n","      <td>0.511437</td>\n","      <td>50.174200</td>\n","      <td>99.653000</td>\n","    </tr>\n","    <tr>\n","      <td>1400</td>\n","      <td>0.343000</td>\n","      <td>0.341468</td>\n","      <td>0.856000</td>\n","      <td>0.513091</td>\n","      <td>0.725679</td>\n","      <td>0.525251</td>\n","      <td>50.194500</td>\n","      <td>99.613000</td>\n","    </tr>\n","    <tr>\n","      <td>1600</td>\n","      <td>0.331800</td>\n","      <td>0.340916</td>\n","      <td>0.859200</td>\n","      <td>0.567608</td>\n","      <td>0.729108</td>\n","      <td>0.557068</td>\n","      <td>50.208200</td>\n","      <td>99.585000</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>0.345800</td>\n","      <td>0.340209</td>\n","      <td>0.859600</td>\n","      <td>0.571429</td>\n","      <td>0.730816</td>\n","      <td>0.559563</td>\n","      <td>50.234100</td>\n","      <td>99.534000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.342600</td>\n","      <td>0.338761</td>\n","      <td>0.856800</td>\n","      <td>0.533718</td>\n","      <td>0.721835</td>\n","      <td>0.536454</td>\n","      <td>50.134500</td>\n","      <td>99.732000</td>\n","    </tr>\n","    <tr>\n","      <td>2200</td>\n","      <td>0.317200</td>\n","      <td>0.347614</td>\n","      <td>0.857800</td>\n","      <td>0.524035</td>\n","      <td>0.749441</td>\n","      <td>0.531390</td>\n","      <td>50.189400</td>\n","      <td>99.623000</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>0.334400</td>\n","      <td>0.337338</td>\n","      <td>0.861400</td>\n","      <td>0.604775</td>\n","      <td>0.729648</td>\n","      <td>0.583215</td>\n","      <td>50.212400</td>\n","      <td>99.577000</td>\n","    </tr>\n","    <tr>\n","      <td>2600</td>\n","      <td>0.353200</td>\n","      <td>0.334653</td>\n","      <td>0.861800</td>\n","      <td>0.579407</td>\n","      <td>0.746858</td>\n","      <td>0.564806</td>\n","      <td>50.214700</td>\n","      <td>99.572000</td>\n","    </tr>\n","    <tr>\n","      <td>2800</td>\n","      <td>0.348100</td>\n","      <td>0.343134</td>\n","      <td>0.858400</td>\n","      <td>0.524443</td>\n","      <td>0.761902</td>\n","      <td>0.531742</td>\n","      <td>50.222500</td>\n","      <td>99.557000</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.349500</td>\n","      <td>0.334191</td>\n","      <td>0.862000</td>\n","      <td>0.606847</td>\n","      <td>0.732807</td>\n","      <td>0.584697</td>\n","      <td>50.089500</td>\n","      <td>99.821000</td>\n","    </tr>\n","    <tr>\n","      <td>3200</td>\n","      <td>0.326200</td>\n","      <td>0.344647</td>\n","      <td>0.859200</td>\n","      <td>0.536502</td>\n","      <td>0.757916</td>\n","      <td>0.538425</td>\n","      <td>50.191200</td>\n","      <td>99.619000</td>\n","    </tr>\n","    <tr>\n","      <td>3400</td>\n","      <td>0.337800</td>\n","      <td>0.341457</td>\n","      <td>0.859400</td>\n","      <td>0.541687</td>\n","      <td>0.754101</td>\n","      <td>0.541367</td>\n","      <td>50.179600</td>\n","      <td>99.642000</td>\n","    </tr>\n","    <tr>\n","      <td>3600</td>\n","      <td>0.353900</td>\n","      <td>0.332835</td>\n","      <td>0.865000</td>\n","      <td>0.624666</td>\n","      <td>0.744203</td>\n","      <td>0.598318</td>\n","      <td>50.157500</td>\n","      <td>99.686000</td>\n","    </tr>\n","    <tr>\n","      <td>3800</td>\n","      <td>0.331900</td>\n","      <td>0.338190</td>\n","      <td>0.860800</td>\n","      <td>0.557274</td>\n","      <td>0.756038</td>\n","      <td>0.550661</td>\n","      <td>50.205100</td>\n","      <td>99.591000</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.332700</td>\n","      <td>0.338830</td>\n","      <td>0.860800</td>\n","      <td>0.555400</td>\n","      <td>0.758256</td>\n","      <td>0.549532</td>\n","      <td>50.167200</td>\n","      <td>99.667000</td>\n","    </tr>\n","    <tr>\n","      <td>4200</td>\n","      <td>0.324800</td>\n","      <td>0.341580</td>\n","      <td>0.859600</td>\n","      <td>0.537818</td>\n","      <td>0.763281</td>\n","      <td>0.539224</td>\n","      <td>50.164600</td>\n","      <td>99.672000</td>\n","    </tr>\n","    <tr>\n","      <td>4400</td>\n","      <td>0.342800</td>\n","      <td>0.333496</td>\n","      <td>0.861000</td>\n","      <td>0.562058</td>\n","      <td>0.753447</td>\n","      <td>0.553603</td>\n","      <td>50.158600</td>\n","      <td>99.684000</td>\n","    </tr>\n","    <tr>\n","      <td>4600</td>\n","      <td>0.316600</td>\n","      <td>0.339423</td>\n","      <td>0.860800</td>\n","      <td>0.535637</td>\n","      <td>0.793010</td>\n","      <td>0.538233</td>\n","      <td>50.146000</td>\n","      <td>99.709000</td>\n","    </tr>\n","    <tr>\n","      <td>4800</td>\n","      <td>0.334000</td>\n","      <td>0.333845</td>\n","      <td>0.861800</td>\n","      <td>0.560917</td>\n","      <td>0.765529</td>\n","      <td>0.552942</td>\n","      <td>50.168000</td>\n","      <td>99.665000</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.314300</td>\n","      <td>0.335961</td>\n","      <td>0.865400</td>\n","      <td>0.592798</td>\n","      <td>0.771483</td>\n","      <td>0.573695</td>\n","      <td>50.166000</td>\n","      <td>99.669000</td>\n","    </tr>\n","    <tr>\n","      <td>5200</td>\n","      <td>0.345800</td>\n","      <td>0.337386</td>\n","      <td>0.861800</td>\n","      <td>0.552424</td>\n","      <td>0.778277</td>\n","      <td>0.547858</td>\n","      <td>50.127500</td>\n","      <td>99.746000</td>\n","    </tr>\n","    <tr>\n","      <td>5400</td>\n","      <td>0.352700</td>\n","      <td>0.342138</td>\n","      <td>0.861000</td>\n","      <td>0.540913</td>\n","      <td>0.785209</td>\n","      <td>0.541175</td>\n","      <td>50.184400</td>\n","      <td>99.633000</td>\n","    </tr>\n","    <tr>\n","      <td>5600</td>\n","      <td>0.338600</td>\n","      <td>0.332432</td>\n","      <td>0.864400</td>\n","      <td>0.585250</td>\n","      <td>0.768907</td>\n","      <td>0.568589</td>\n","      <td>50.181800</td>\n","      <td>99.638000</td>\n","    </tr>\n","    <tr>\n","      <td>5800</td>\n","      <td>0.347900</td>\n","      <td>0.334322</td>\n","      <td>0.862800</td>\n","      <td>0.563635</td>\n","      <td>0.776236</td>\n","      <td>0.554658</td>\n","      <td>50.181200</td>\n","      <td>99.639000</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.332000</td>\n","      <td>0.335249</td>\n","      <td>0.862400</td>\n","      <td>0.567818</td>\n","      <td>0.765002</td>\n","      <td>0.557248</td>\n","      <td>50.159800</td>\n","      <td>99.681000</td>\n","    </tr>\n","    <tr>\n","      <td>6200</td>\n","      <td>0.338600</td>\n","      <td>0.339029</td>\n","      <td>0.861400</td>\n","      <td>0.538155</td>\n","      <td>0.800801</td>\n","      <td>0.539714</td>\n","      <td>50.148300</td>\n","      <td>99.704000</td>\n","    </tr>\n","    <tr>\n","      <td>6400</td>\n","      <td>0.328300</td>\n","      <td>0.335536</td>\n","      <td>0.862800</td>\n","      <td>0.552284</td>\n","      <td>0.796894</td>\n","      <td>0.547879</td>\n","      <td>50.154400</td>\n","      <td>99.692000</td>\n","    </tr>\n","    <tr>\n","      <td>6600</td>\n","      <td>0.325300</td>\n","      <td>0.337825</td>\n","      <td>0.862800</td>\n","      <td>0.549342</td>\n","      <td>0.803726</td>\n","      <td>0.546184</td>\n","      <td>50.134300</td>\n","      <td>99.732000</td>\n","    </tr>\n","    <tr>\n","      <td>6800</td>\n","      <td>0.326200</td>\n","      <td>0.347575</td>\n","      <td>0.860600</td>\n","      <td>0.525953</td>\n","      <td>0.817251</td>\n","      <td>0.533031</td>\n","      <td>50.192400</td>\n","      <td>99.617000</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.323200</td>\n","      <td>0.332255</td>\n","      <td>0.863600</td>\n","      <td>0.571587</td>\n","      <td>0.776102</td>\n","      <td>0.559646</td>\n","      <td>50.149500</td>\n","      <td>99.702000</td>\n","    </tr>\n","    <tr>\n","      <td>7200</td>\n","      <td>0.332200</td>\n","      <td>0.342789</td>\n","      <td>0.862400</td>\n","      <td>0.541997</td>\n","      <td>0.814398</td>\n","      <td>0.541995</td>\n","      <td>50.130800</td>\n","      <td>99.739000</td>\n","    </tr>\n","    <tr>\n","      <td>7400</td>\n","      <td>0.345100</td>\n","      <td>0.332910</td>\n","      <td>0.861200</td>\n","      <td>0.559474</td>\n","      <td>0.758963</td>\n","      <td>0.552026</td>\n","      <td>50.160800</td>\n","      <td>99.679000</td>\n","    </tr>\n","    <tr>\n","      <td>7600</td>\n","      <td>0.331000</td>\n","      <td>0.349868</td>\n","      <td>0.862000</td>\n","      <td>0.538610</td>\n","      <td>0.814511</td>\n","      <td>0.540066</td>\n","      <td>50.172300</td>\n","      <td>99.657000</td>\n","    </tr>\n","    <tr>\n","      <td>7800</td>\n","      <td>0.327700</td>\n","      <td>0.336587</td>\n","      <td>0.864400</td>\n","      <td>0.590179</td>\n","      <td>0.763719</td>\n","      <td>0.571979</td>\n","      <td>50.189500</td>\n","      <td>99.622000</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.319000</td>\n","      <td>0.333240</td>\n","      <td>0.864000</td>\n","      <td>0.588970</td>\n","      <td>0.760791</td>\n","      <td>0.571180</td>\n","      <td>50.162700</td>\n","      <td>99.676000</td>\n","    </tr>\n","    <tr>\n","      <td>8200</td>\n","      <td>0.330700</td>\n","      <td>0.331954</td>\n","      <td>0.865000</td>\n","      <td>0.593199</td>\n","      <td>0.766863</td>\n","      <td>0.574025</td>\n","      <td>50.163800</td>\n","      <td>99.673000</td>\n","    </tr>\n","    <tr>\n","      <td>8400</td>\n","      <td>0.330200</td>\n","      <td>0.335797</td>\n","      <td>0.864400</td>\n","      <td>0.571431</td>\n","      <td>0.787737</td>\n","      <td>0.559550</td>\n","      <td>50.193700</td>\n","      <td>99.614000</td>\n","    </tr>\n","    <tr>\n","      <td>8600</td>\n","      <td>0.345000</td>\n","      <td>0.332384</td>\n","      <td>0.866400</td>\n","      <td>0.621466</td>\n","      <td>0.755548</td>\n","      <td>0.595184</td>\n","      <td>50.211100</td>\n","      <td>99.580000</td>\n","    </tr>\n","    <tr>\n","      <td>8800</td>\n","      <td>0.312200</td>\n","      <td>0.339049</td>\n","      <td>0.861400</td>\n","      <td>0.554008</td>\n","      <td>0.769151</td>\n","      <td>0.548753</td>\n","      <td>50.192900</td>\n","      <td>99.616000</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.331600</td>\n","      <td>0.335726</td>\n","      <td>0.863800</td>\n","      <td>0.573542</td>\n","      <td>0.776090</td>\n","      <td>0.560893</td>\n","      <td>50.157900</td>\n","      <td>99.685000</td>\n","    </tr>\n","    <tr>\n","      <td>9200</td>\n","      <td>0.342700</td>\n","      <td>0.328464</td>\n","      <td>0.865200</td>\n","      <td>0.637116</td>\n","      <td>0.739919</td>\n","      <td>0.609169</td>\n","      <td>50.139000</td>\n","      <td>99.723000</td>\n","    </tr>\n","    <tr>\n","      <td>9400</td>\n","      <td>0.348900</td>\n","      <td>0.340552</td>\n","      <td>0.863600</td>\n","      <td>0.553924</td>\n","      <td>0.809183</td>\n","      <td>0.548912</td>\n","      <td>50.154700</td>\n","      <td>99.692000</td>\n","    </tr>\n","    <tr>\n","      <td>9600</td>\n","      <td>0.334200</td>\n","      <td>0.337093</td>\n","      <td>0.862600</td>\n","      <td>0.550164</td>\n","      <td>0.797653</td>\n","      <td>0.546632</td>\n","      <td>50.204100</td>\n","      <td>99.593000</td>\n","    </tr>\n","    <tr>\n","      <td>9800</td>\n","      <td>0.344300</td>\n","      <td>0.338978</td>\n","      <td>0.863200</td>\n","      <td>0.552616</td>\n","      <td>0.804131</td>\n","      <td>0.548113</td>\n","      <td>50.151500</td>\n","      <td>99.698000</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.325600</td>\n","      <td>0.332279</td>\n","      <td>0.864000</td>\n","      <td>0.575484</td>\n","      <td>0.776086</td>\n","      <td>0.562140</td>\n","      <td>50.143200</td>\n","      <td>99.714000</td>\n","    </tr>\n","    <tr>\n","      <td>10200</td>\n","      <td>0.333300</td>\n","      <td>0.336960</td>\n","      <td>0.863200</td>\n","      <td>0.570331</td>\n","      <td>0.772402</td>\n","      <td>0.558847</td>\n","      <td>50.123900</td>\n","      <td>99.753000</td>\n","    </tr>\n","    <tr>\n","      <td>10400</td>\n","      <td>0.339500</td>\n","      <td>0.335314</td>\n","      <td>0.863600</td>\n","      <td>0.572474</td>\n","      <td>0.774877</td>\n","      <td>0.560211</td>\n","      <td>50.154400</td>\n","      <td>99.692000</td>\n","    </tr>\n","    <tr>\n","      <td>10600</td>\n","      <td>0.347900</td>\n","      <td>0.331729</td>\n","      <td>0.863000</td>\n","      <td>0.576283</td>\n","      <td>0.762714</td>\n","      <td>0.562684</td>\n","      <td>50.182300</td>\n","      <td>99.637000</td>\n","    </tr>\n","    <tr>\n","      <td>10800</td>\n","      <td>0.308000</td>\n","      <td>0.339254</td>\n","      <td>0.864800</td>\n","      <td>0.569088</td>\n","      <td>0.798154</td>\n","      <td>0.558090</td>\n","      <td>50.143000</td>\n","      <td>99.715000</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.339700</td>\n","      <td>0.330173</td>\n","      <td>0.865600</td>\n","      <td>0.616399</td>\n","      <td>0.753040</td>\n","      <td>0.591325</td>\n","      <td>50.124400</td>\n","      <td>99.752000</td>\n","    </tr>\n","    <tr>\n","      <td>11200</td>\n","      <td>0.338100</td>\n","      <td>0.331394</td>\n","      <td>0.864400</td>\n","      <td>0.575859</td>\n","      <td>0.780887</td>\n","      <td>0.562375</td>\n","      <td>50.144900</td>\n","      <td>99.711000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='1876' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [313/313 05:01]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"g5yMbjcAM2Ar","colab":{"base_uri":"https://localhost:8080/","height":832},"executionInfo":{"status":"ok","timestamp":1619935071774,"user_tz":-480,"elapsed":1383,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}},"outputId":"69b13b3e-efb2-4ae8-fc5e-f73ee5a7c058"},"source":["display(dev_df)\n","display(dev_df.describe())\n","\n","display(test_df)\n","display(test_df.describe())"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>test_loss</th>\n","      <th>test_accuracy</th>\n","      <th>test_f1</th>\n","      <th>test_precision</th>\n","      <th>test_recall</th>\n","      <th>test_runtime</th>\n","      <th>test_samples_per_second</th>\n","      <th>test_mem_cpu_alloc_delta</th>\n","      <th>test_mem_gpu_alloc_delta</th>\n","      <th>test_mem_cpu_peaked_delta</th>\n","      <th>test_mem_gpu_peaked_delta</th>\n","      <th>seed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.330536</td>\n","      <td>0.8654</td>\n","      <td>0.605256</td>\n","      <td>0.759615</td>\n","      <td>0.582734</td>\n","      <td>50.0774</td>\n","      <td>99.846</td>\n","      <td>1855488.0</td>\n","      <td>-132096.0</td>\n","      <td>0.0</td>\n","      <td>780432896.0</td>\n","      <td>42.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>0.329735</td>\n","      <td>0.8652</td>\n","      <td>0.600479</td>\n","      <td>0.761892</td>\n","      <td>0.579227</td>\n","      <td>50.0630</td>\n","      <td>99.874</td>\n","      <td>-8192.0</td>\n","      <td>-132096.0</td>\n","      <td>8192.0</td>\n","      <td>783573504.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>0.328464</td>\n","      <td>0.8652</td>\n","      <td>0.637116</td>\n","      <td>0.739919</td>\n","      <td>0.609169</td>\n","      <td>50.0607</td>\n","      <td>99.879</td>\n","      <td>0.0</td>\n","      <td>-132096.0</td>\n","      <td>0.0</td>\n","      <td>781954560.0</td>\n","      <td>2.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   test_loss  test_accuracy  ...  test_mem_gpu_peaked_delta  seed\n","0   0.330536         0.8654  ...                780432896.0  42.0\n","0   0.329735         0.8652  ...                783573504.0   1.0\n","0   0.328464         0.8652  ...                781954560.0   2.0\n","\n","[3 rows x 12 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>test_loss</th>\n","      <th>test_accuracy</th>\n","      <th>test_f1</th>\n","      <th>test_precision</th>\n","      <th>test_recall</th>\n","      <th>test_runtime</th>\n","      <th>test_samples_per_second</th>\n","      <th>test_mem_cpu_alloc_delta</th>\n","      <th>test_mem_gpu_alloc_delta</th>\n","      <th>test_mem_cpu_peaked_delta</th>\n","      <th>test_mem_gpu_peaked_delta</th>\n","      <th>seed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000e+00</td>\n","      <td>3.0</td>\n","      <td>3.000000</td>\n","      <td>3.000000e+00</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.329578</td>\n","      <td>0.865267</td>\n","      <td>0.614284</td>\n","      <td>0.753809</td>\n","      <td>0.590377</td>\n","      <td>50.067033</td>\n","      <td>99.866333</td>\n","      <td>6.157653e+05</td>\n","      <td>-132096.0</td>\n","      <td>2730.666667</td>\n","      <td>7.819870e+08</td>\n","      <td>15.000000</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.001045</td>\n","      <td>0.000115</td>\n","      <td>0.019917</td>\n","      <td>0.012083</td>\n","      <td>0.016369</td>\n","      <td>0.009051</td>\n","      <td>0.017786</td>\n","      <td>1.073639e+06</td>\n","      <td>0.0</td>\n","      <td>4729.653405</td>\n","      <td>1.570555e+06</td>\n","      <td>23.388031</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.328464</td>\n","      <td>0.865200</td>\n","      <td>0.600479</td>\n","      <td>0.739919</td>\n","      <td>0.579227</td>\n","      <td>50.060700</td>\n","      <td>99.846000</td>\n","      <td>-8.192000e+03</td>\n","      <td>-132096.0</td>\n","      <td>0.000000</td>\n","      <td>7.804329e+08</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.329099</td>\n","      <td>0.865200</td>\n","      <td>0.602868</td>\n","      <td>0.749767</td>\n","      <td>0.580981</td>\n","      <td>50.061850</td>\n","      <td>99.860000</td>\n","      <td>-4.096000e+03</td>\n","      <td>-132096.0</td>\n","      <td>0.000000</td>\n","      <td>7.811937e+08</td>\n","      <td>1.500000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.329735</td>\n","      <td>0.865200</td>\n","      <td>0.605256</td>\n","      <td>0.759615</td>\n","      <td>0.582734</td>\n","      <td>50.063000</td>\n","      <td>99.874000</td>\n","      <td>0.000000e+00</td>\n","      <td>-132096.0</td>\n","      <td>0.000000</td>\n","      <td>7.819546e+08</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.330135</td>\n","      <td>0.865300</td>\n","      <td>0.621186</td>\n","      <td>0.760754</td>\n","      <td>0.595952</td>\n","      <td>50.070200</td>\n","      <td>99.876500</td>\n","      <td>9.277440e+05</td>\n","      <td>-132096.0</td>\n","      <td>4096.000000</td>\n","      <td>7.827640e+08</td>\n","      <td>22.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.330536</td>\n","      <td>0.865400</td>\n","      <td>0.637116</td>\n","      <td>0.761892</td>\n","      <td>0.609169</td>\n","      <td>50.077400</td>\n","      <td>99.879000</td>\n","      <td>1.855488e+06</td>\n","      <td>-132096.0</td>\n","      <td>8192.000000</td>\n","      <td>7.835735e+08</td>\n","      <td>42.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       test_loss  test_accuracy  ...  test_mem_gpu_peaked_delta       seed\n","count   3.000000       3.000000  ...               3.000000e+00   3.000000\n","mean    0.329578       0.865267  ...               7.819870e+08  15.000000\n","std     0.001045       0.000115  ...               1.570555e+06  23.388031\n","min     0.328464       0.865200  ...               7.804329e+08   1.000000\n","25%     0.329099       0.865200  ...               7.811937e+08   1.500000\n","50%     0.329735       0.865200  ...               7.819546e+08   2.000000\n","75%     0.330135       0.865300  ...               7.827640e+08  22.000000\n","max     0.330536       0.865400  ...               7.835735e+08  42.000000\n","\n","[8 rows x 12 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>test_loss</th>\n","      <th>test_accuracy</th>\n","      <th>test_f1</th>\n","      <th>test_precision</th>\n","      <th>test_recall</th>\n","      <th>test_runtime</th>\n","      <th>test_samples_per_second</th>\n","      <th>test_mem_cpu_alloc_delta</th>\n","      <th>test_mem_gpu_alloc_delta</th>\n","      <th>test_mem_cpu_peaked_delta</th>\n","      <th>test_mem_gpu_peaked_delta</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.326944</td>\n","      <td>0.86164</td>\n","      <td>0.591425</td>\n","      <td>0.737534</td>\n","      <td>0.573187</td>\n","      <td>250.7776</td>\n","      <td>99.690</td>\n","      <td>2527232.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>832598528.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>0.325663</td>\n","      <td>0.86288</td>\n","      <td>0.588189</td>\n","      <td>0.750501</td>\n","      <td>0.570749</td>\n","      <td>250.7069</td>\n","      <td>99.718</td>\n","      <td>2248704.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>805697536.0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>0.323045</td>\n","      <td>0.86264</td>\n","      <td>0.619391</td>\n","      <td>0.732030</td>\n","      <td>0.594676</td>\n","      <td>250.6013</td>\n","      <td>99.760</td>\n","      <td>1638400.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>833975296.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   test_loss  ...  test_mem_gpu_peaked_delta\n","0   0.326944  ...                832598528.0\n","0   0.325663  ...                805697536.0\n","0   0.323045  ...                833975296.0\n","\n","[3 rows x 11 columns]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>test_loss</th>\n","      <th>test_accuracy</th>\n","      <th>test_f1</th>\n","      <th>test_precision</th>\n","      <th>test_recall</th>\n","      <th>test_runtime</th>\n","      <th>test_samples_per_second</th>\n","      <th>test_mem_cpu_alloc_delta</th>\n","      <th>test_mem_gpu_alloc_delta</th>\n","      <th>test_mem_cpu_peaked_delta</th>\n","      <th>test_mem_gpu_peaked_delta</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000e+00</td>\n","      <td>3.0</td>\n","      <td>3.0</td>\n","      <td>3.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.325217</td>\n","      <td>0.862387</td>\n","      <td>0.599669</td>\n","      <td>0.740022</td>\n","      <td>0.579537</td>\n","      <td>250.695267</td>\n","      <td>99.722667</td>\n","      <td>2.138112e+06</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.240905e+08</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.001988</td>\n","      <td>0.000658</td>\n","      <td>0.017157</td>\n","      <td>0.009484</td>\n","      <td>0.013167</td>\n","      <td>0.088724</td>\n","      <td>0.035233</td>\n","      <td>4.546191e+05</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.594360e+07</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.323045</td>\n","      <td>0.861640</td>\n","      <td>0.588189</td>\n","      <td>0.732030</td>\n","      <td>0.570749</td>\n","      <td>250.601300</td>\n","      <td>99.690000</td>\n","      <td>1.638400e+06</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.056975e+08</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.324354</td>\n","      <td>0.862140</td>\n","      <td>0.589807</td>\n","      <td>0.734782</td>\n","      <td>0.571968</td>\n","      <td>250.654100</td>\n","      <td>99.704000</td>\n","      <td>1.943552e+06</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.191480e+08</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.325663</td>\n","      <td>0.862640</td>\n","      <td>0.591425</td>\n","      <td>0.737534</td>\n","      <td>0.573187</td>\n","      <td>250.706900</td>\n","      <td>99.718000</td>\n","      <td>2.248704e+06</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.325985e+08</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.326304</td>\n","      <td>0.862760</td>\n","      <td>0.605408</td>\n","      <td>0.744018</td>\n","      <td>0.583931</td>\n","      <td>250.742250</td>\n","      <td>99.739000</td>\n","      <td>2.387968e+06</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.332869e+08</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.326944</td>\n","      <td>0.862880</td>\n","      <td>0.619391</td>\n","      <td>0.750501</td>\n","      <td>0.594676</td>\n","      <td>250.777600</td>\n","      <td>99.760000</td>\n","      <td>2.527232e+06</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.339753e+08</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       test_loss  ...  test_mem_gpu_peaked_delta\n","count   3.000000  ...               3.000000e+00\n","mean    0.325217  ...               8.240905e+08\n","std     0.001988  ...               1.594360e+07\n","min     0.323045  ...               8.056975e+08\n","25%     0.324354  ...               8.191480e+08\n","50%     0.325663  ...               8.325985e+08\n","75%     0.326304  ...               8.332869e+08\n","max     0.326944  ...               8.339753e+08\n","\n","[8 rows x 11 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"0pHUcyA9gIfL"},"source":["Saving results"]},{"cell_type":"code","metadata":{"id":"dmgfsM3OM3wT","executionInfo":{"status":"aborted","timestamp":1619923224713,"user_tz":-480,"elapsed":508254,"user":{"displayName":"Ng WX","photoUrl":"https://lh4.googleusercontent.com/-AXn_6O-ootU/AAAAAAAAAAI/AAAAAAAABCs/TtxB7rBRgS4/s64/photo.jpg","userId":"17422648075664283449"}}},"source":["\n"],"execution_count":null,"outputs":[]}]}